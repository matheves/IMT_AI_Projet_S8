#Importing OpenAI gym package and MuJoCo engine
import gym
from gym.wrappers import RecordVideo
import tensorflow as tf
from tensorflow import keras
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
import time
import os
from collections import deque
import matplotlib.animation as animation


replay_memory = deque(maxlen=2000)

num_steps = 1000
num_iters = 100
max_epsilon = 50

env = gym.make('CartPole-v1')


#env = gym.make("Walker2d-v2")


isDocker = os.environ.get("DOCKER_FLAG") == "1"
#isDocker = True


###Deep q-Learning
keras.backend.clear_session()

input_shape = [4] # == env.observation_space.shape
n_outputs = 2 # == env.action_space.n

model = keras.models.Sequential([
    keras.layers.Dense(32, activation="elu", input_shape=input_shape),
    keras.layers.Dense(32, activation="elu"),
    keras.layers.Dense(n_outputs)
])



liste_fibonnaci = [1,1]
actual_fibonnaci = liste_fibonnaci[-1]

def next_fibonnaci(liste):
    return liste[-1]+liste[-2]
    #usefull for rec video


def epsilon_greedy_policy(state, epsilon=0):
    if np.random.rand() < epsilon:
        return np.random.randint(n_outputs)
    else:
        Q_values = model.predict(state[np.newaxis])
        return np.argmax(Q_values[0])

def sample_experiences(batch_size):
    indices = np.random.randint(len(replay_memory), size=batch_size)
    batch = [replay_memory[index] for index in indices]
    states, actions, rewards, next_states, dones = [
        np.array([experience[field_index] for experience in batch])
        for field_index in range(5)]
    return states, actions, rewards, next_states, dones

def play_one_step(env, state, epsilon):
    action = epsilon_greedy_policy(state, epsilon)
    next_state, reward, done, info = env.step(action)
    replay_memory.append((state, action, reward, next_state, done))
    return next_state, reward, done, info


batch_size = 32
discount_rate = 0.95
optimizer = keras.optimizers.Adam(learning_rate=1e-2)
loss_fn = keras.losses.mean_squared_error

def training_step(batch_size):
    experiences = sample_experiences(batch_size)
    states, actions, rewards, next_states, dones = experiences
    next_Q_values = model.predict(next_states)
    max_next_Q_values = np.max(next_Q_values, axis=1)
    target_Q_values = (rewards +
                       (1 - dones) * discount_rate * max_next_Q_values)
    target_Q_values = target_Q_values.reshape(-1, 1)
    mask = tf.one_hot(actions, n_outputs)
    with tf.GradientTape() as tape:
        all_Q_values = model(states)
        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)
        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
###



#env.seed(42)
#np.random.seed(42)
#tf.random.set_seed(42)

rewards = [] 
best_score = 0

obs = env.reset()

fig, ax = plt.subplots()
plt.axis("off")
renders = []
for episode in range(1, num_iters+1):
    obs = env.reset()
    #print("Itteration ", episode)
    is_record = False
    if episode == actual_fibonnaci:
        liste_fibonnaci.append(next_fibonnaci(liste_fibonnaci))
        actual_fibonnaci = liste_fibonnaci[-1]
        is_record = True
    for step in range(num_steps):
        epsilon = max(1 - episode / max_epsilon, 0.01)
        obs, reward, done, info = play_one_step(env, obs, epsilon)
        
        if is_record:
            #ax.text(20,20, str(episode))
            episode_text = ax.text(20, 20, "Episode " + str(episode) + "  Step " + str(step))
            im = ax.imshow(env.render(mode='rgb_array'), animated=True)
            renders.append([im, episode_text])
            
        if done:
            break
    rewards.append(step) # Not shown in the book
    if step >= best_score: # Not shown
        best_weights = model.get_weights() # Not shown
        best_score = step # Not shown
        #print("\nNew best score: ", best_score)
    #print("\rEpisode: {}, Steps: {}, eps: {:.3f}".format(episode, step + 1, epsilon), end="") # Not shown
    if episode > 50:
        training_step(batch_size)
    env.reset()
print("\nBest score: ", best_score)
env.close()


ani = animation.ArtistAnimation(fig, renders, interval=50, blit=True, repeat=False)
ani.save("mov.mp4")

plt.clf()
plt.figure(figsize=(8, 4))
plt.plot(rewards)
plt.xlabel("Episode", fontsize=14)
plt.ylabel("Sum of rewards", fontsize=14)
plt.show()
plt.savefig("stats.png")
