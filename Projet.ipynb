{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe15ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing OpenAI gym package and MuJoCo engine\n",
    "#import gym\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments.wrappers import ActionDiscretizeWrapper\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from collections import deque\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "replay_memory = deque(maxlen=2000)\n",
    "\n",
    "num_steps = 1000\n",
    "num_iters = 100\n",
    "max_epsilon = 50\n",
    "\n",
    "\n",
    "env_name = 'InvertedPendulum-v2'\n",
    "env = suite_gym.load(env_name)\n",
    "env = ActionDiscretizeWrapper(env, num_actions=5)\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "\n",
    "#train_py_env = suite_gym.load(\"InvertedPendulum-v2\")\n",
    "#train_py_env = ActionDiscretizeWrapper(train_py_env, num_actions=5)\n",
    "#train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "\n",
    "#eval_py_env  = suite_gym.load(\"InvertedPendulum-v2\")\n",
    "#eval_py_env = ActionDiscretizeWrapper(eval_py_env, num_actions=5)\n",
    "#eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "# Version site tensorflow\n",
    "\n",
    "#num_iterations = 200 # @param {type:\"integer\"}\n",
    "\n",
    "#initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "#collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
    "#replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "#batch_size = 64  # @param {type:\"integer\"}\n",
    "#learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "#log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "#num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "#eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effbe849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.utils import common\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "    return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510374e5",
   "metadata": {},
   "source": [
    "# Version livre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df70234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buffer\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100000) # reduce if OOM error\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5997d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b033f6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 0\n",
      "\t\t EnvironmentSteps = 0\n",
      "\t\t AverageReturn = 0.0\n",
      "\t\t AverageEpisodeLength = 0.0\n"
     ]
    }
   ],
   "source": [
    "#Metric\n",
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]\n",
    "\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "496505f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buffer\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=tf_env.batch_size,\n",
    "    max_length=100000) # reduce if OOM error\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31a06bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/20000"
     ]
    }
   ],
   "source": [
    "#collect\n",
    "update_period = 4\n",
    "\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration\n",
    "\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                        tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n",
    "    num_steps=10000) # <=> 80,000 ALE frames\n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef39fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)\n",
    "\n",
    "from tf_agents.utils.common import function\n",
    "\n",
    "#collect_driver.run = function(collect_driver.run)\n",
    "#agent.train = function(agent.train)\n",
    "\n",
    "def train_agent(n_iterations):\n",
    "\n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 1000 == 0:\n",
    "            log_metrics(train_metrics)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6f26831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/.local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/.local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "INFO:absl: \n",
      "\t\t NumberOfEpisodes = 4\n",
      "\t\t EnvironmentSteps = 12\n",
      "\t\t AverageReturn = 3.0\n",
      "\t\t AverageEpisodeLength = 2.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 loss:31.47085"
     ]
    }
   ],
   "source": [
    "train_agent(n_iterations=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de87543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900/1000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.animation:Animation.save using <class 'matplotlib.animation.FFMpegWriter'>\n",
      "INFO:matplotlib.animation:MovieWriter._run: running command: ffmpeg -f rawvideo -vcodec rawvideo -s 640x480 -pix_fmt rgba -r 25.0 -loglevel error -i pipe: -vcodec h264 -pix_fmt yuv420p -y test.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000"
     ]
    }
   ],
   "source": [
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, ShowProgress(1000)],\n",
    "    num_steps=1000)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "plot_animation(frames).save(\"test.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37cc9801",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'MP4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4949/3208158479.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                      \u001b[0msave_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                      \u001b[0mduration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                      loop=0)\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/railrl/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2225\u001b[0m             \u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_all\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2227\u001b[0;31m             \u001b[0msave_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAVE_ALL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m             \u001b[0msave_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSAVE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MP4'"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "\n",
    "image_path = \"test.mp4\"\n",
    "frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "frame_images[0].save(image_path, format='MP4',\n",
    "                     append_images=frame_images[1:],\n",
    "                     save_all=True,\n",
    "                     duration=30,\n",
    "                     loop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266d866",
   "metadata": {},
   "source": [
    "# Version site tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d670e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "        total_return += episode_return\n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657b2d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmpdac0g9i0.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:567] Loading latest checkpoint from /tmp/tmpdac0g9i0\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 45375\n"
     ]
    }
   ],
   "source": [
    "#tampon relecture\n",
    "\n",
    "import reverb\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "\n",
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_max_length,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  replay_buffer.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f039942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.00265241, -0.00396902, -0.00252635, -0.00246298]),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(0, dtype=int32)})\n"
     ]
    }
   ],
   "source": [
    "# Collecte de données\n",
    "\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "\n",
    "py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      random_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=initial_collect_steps).run(env.reset())\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a57dcd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supports batched time steps with a single batch dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4456/3826734029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Collect a few steps and save to the replay buffer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/drivers/py_driver.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m       \u001b[0mnext_time_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/policies/py_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/policies/py_tf_eager_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mpolicy_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy_action_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m       \u001b[0mpolicy_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy_action_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_time_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/policies/epsilon_greedy_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mouter_ndims\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m       raise ValueError(\n\u001b[0;32m--> 131\u001b[0;31m           'Only supports batched time steps with a single batch dimension')\n\u001b[0m\u001b[1;32m    132\u001b[0m     action = tf.nest.map_structure(lambda g, r: tf.compat.v1.where(cond, g, r),\n\u001b[1;32m    133\u001b[0m                                    greedy_action.action, random_action.action)\n",
      "\u001b[0;31mValueError\u001b[0m: Only supports batched time steps with a single batch dimension"
     ]
    }
   ],
   "source": [
    "# Formation de l'agent\n",
    "\n",
    "time_step = env.reset()\n",
    "try:\n",
    "    %%time\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step.\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "# Reset the environment.\n",
    "time_step = train_env.reset()\n",
    "\n",
    "# Create a driver to collect experience.\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    train_env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      agent.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=collect_steps_per_iteration)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Collect a few steps and save to the replay buffer.\n",
    "    time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741a3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7db57a",
   "metadata": {},
   "source": [
    "# OLD DONT WORK LAYER INIT AGENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "215fcbe6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"EncodingNetwork\" \"                 f\"(type EncodingNetwork).\n\nInput 0 of layer \"conv2d_6\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (1, 4)\n\nCall arguments received by layer \"EncodingNetwork\" \"                 f\"(type EncodingNetwork):\n  • observation=tf.Tensor(shape=(1, 4), dtype=float64)\n  • step_type=tf.Tensor(shape=(1,), dtype=int32)\n  • network_state=()\n  • training=False\n  In call to configurable 'DqnAgent' (<class 'tf_agents.agents.dqn.dqn_agent.DqnAgent'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3987/2302721851.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                  \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# discount factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                  \u001b[0mtrain_step_counter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                  epsilon_greedy=lambda: epsilon_fn(train_step))\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mscope_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" in scope '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_or_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m       \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_exception_message_and_reraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mgin_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gin/utils.py\u001b[0m in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0mproxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mExceptionProxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/gin/config.py\u001b[0m in \u001b[0;36mgin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m       \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/agents/dqn/dqn_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, time_step_spec, action_spec, q_network, optimizer, observation_and_action_constraint_splitter, epsilon_greedy, n_step_update, boltzmann_temperature, emit_log_probability, target_q_network, target_update_tau, target_update_period, td_errors_loss_fn, gamma, reward_scale_factor, gradient_clipping, debug_summaries, summarize_grads_and_vars, train_step_counter, training_data_spec, name)\u001b[0m\n\u001b[1;32m    237\u001b[0m       net_observation_spec, _ = observation_and_action_constraint_splitter(\n\u001b[1;32m    238\u001b[0m           net_observation_spec)\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0mq_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_observation_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_q_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m       \u001b[0mtarget_q_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_observation_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36mcreate_variables\u001b[0;34m(self, input_tensor_spec, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mstep_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mnetwork_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calc_unbatched_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0mnormalized_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"network_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnormalized_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error  # typed-keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     nest_utils.assert_matching_dtypes_and_inner_shapes(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/networks/q_network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, observation, step_type, network_state, training)\u001b[0m\n\u001b[1;32m    147\u001b[0m     state, network_state = self._encoder(\n\u001b[1;32m    148\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         training=training)\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_q_value_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0mnormalized_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"network_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnormalized_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=attribute-error  # typed-keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     nest_utils.assert_matching_dtypes_and_inner_shapes(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tf_agents/networks/encoding_network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_postprocessing_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m       \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_squash\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"EncodingNetwork\" \"                 f\"(type EncodingNetwork).\n\nInput 0 of layer \"conv2d_6\" is incompatible with the layer: expected min_ndim=4, found ndim=2. Full shape received: (1, 4)\n\nCall arguments received by layer \"EncodingNetwork\" \"                 f\"(type EncodingNetwork):\n  • observation=tf.Tensor(shape=(1, 4), dtype=float64)\n  • step_type=tf.Tensor(shape=(1,), dtype=int32)\n  • network_state=()\n  • training=False\n  In call to configurable 'DqnAgent' (<class 'tf_agents.agents.dqn.dqn_agent.DqnAgent'>)"
     ]
    }
   ],
   "source": [
    "## OLD BUT DON T WORK\n",
    "## OLD BUT DON T WORK\n",
    "## OLD BUT DON T WORK\n",
    "## OLD BUT DON T WORK\n",
    "## OLD BUT DON T WORK\n",
    "\n",
    "preprocessing_layer = keras.layers.Lambda(\n",
    "                          lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[512]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0,\n",
    "                                     epsilon=0.00001, centered=True)\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=1.0, # initial ε\n",
    "    decay_steps=250000 // update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate=0.01) # final ε\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "                 tf_env.action_spec(),\n",
    "                 q_network=q_net,\n",
    "                 optimizer=optimizer,\n",
    "                 target_update_period=2000, # <=> 32,000 ALE frames\n",
    "                 td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"),\n",
    "                 gamma=0.99, # discount factor\n",
    "                 train_step_counter=train_step,\n",
    "                 epsilon_greedy=lambda: epsilon_fn(train_step))\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de34ce6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1fbb113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "renders = []\n",
    "for _ in range(10):\n",
    "    time_step = env.step(3) # LEFT\n",
    "    im = ax.imshow(env.render(mode='rgb_array'), animated=True)\n",
    "    renders.append([im, episode_text])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, renders, interval=50, blit=True, repeat=False)\n",
    "ani.save(\"test.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294574be",
   "metadata": {},
   "source": [
    "# Ancien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8711429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 12:33:57.733512: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/envs/railrl/lib/python3.7/site-packages/cv2/../../lib64:/mujoco/.mujoco/mujoco210/bin:/mujoco/.mujoco/mujoco200_linux/bin:/mujoco/.mujoco/mujoco200/bin:/mujoco/.mujoco/mjpro150/bin:/usr/local/nvidia/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/i386-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/root/.mujoco/mujoco210/bin\n",
      "2022-09-23 12:33:57.733554: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-09-23 12:33:57.733573: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (7bf2a36b10a7): /proc/driver/nvidia/version does not exist\n",
      "2022-09-23 12:33:57.733863: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3987/714757990.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_record\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3987/714757990.py\u001b[0m in \u001b[0;36mplay_one_step\u001b[0;34m(env, state, epsilon)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_greedy_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3987/714757990.py\u001b[0m in \u001b[0;36mepsilon_greedy_policy\u001b[0;34m(state, epsilon)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mQ_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not NoneType"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAYTUlEQVR4nO3dyXMjWX4f8B+QAMGtqmtquqtaY4U08liaUOigi3TxWf+3Tr756FA4HKEZ2yNZo251VXUtLGLL9CGZj8lkgsTG7fHzicggCICZCQTxvnhrDqqqqgIAImL40CcAwOMhFABIhAIAiVAAIBEKACRCAYBEKACQCAUAktG6TxwMBnd5HgDcsXXmKqspAJAIBQASoQBAIhQASIQCAIlQACARCgAkQgGARCgAkAgFABKhAEAiFABIhAIAiVAAIBEKACRCAYBEKACQCAUAEqEAQCIUAEiEAgCJUAAgEQoAJEIBgEQoAJAIBQASoQBAIhQASIQCAIlQACARCgAkQgGARCgAkAgFABKhAEAiFABIhAIAiVAAIBEKACRCAYBEKACQCAUAEqEAQCIUAEiEAgCJUAAgEQoAJEIBgEQoAJAIBQASoQBAIhQASEYPfQLwaAwG/fdX1f2eBzwgocDzNhxGHB9HfP99xMuX9e9NOJRlxHwe8fvf17fPzgQE2RtU1Xr/5YNV36LgKZpMIl69ijg5iXj7NmI0qgOhCYXmY1GWEYtFHQ5//GPEhw8RHz8+5JnD1tYp7oUCz89kEvGb30S8eFHfbgKhKOpAaEKh2coyYrmsg+HLl4jf/a4OBrUGnhihAF1HRxF/9VcRp6cRBwd1IBTF1VpCW1nWhf9yWdcYFouI6TTi/fu6WWk2e5jXAVtYp7jXp8DzcXwc8Rd/cTUQ2qHQ1BLazUfDYR0M7cci6qanP/3TiD/8oa5BQCaEAs/DaBTx539edyaPx5eB0O1L6NYUqqr//rKMeP064t27up8BMmGeAs/Dixf11q4dtJuN2lvfY819zTYe1/0Rv/513VkNmRAK5O/0tG7qaTcVtZuMLgr+4WhUb0URw6KIQTsUWs9L22hUN0O9ffvQrxD2RvMR+Xv5si682zWApkno4vZgOIzxwUEdCMNhlGUZ89ksyrKMst2E1PxNUdSdz6NR3Vfx4kXEp08P/UphZ0KBvI1GEYeH/X0HF9tgOIxiNIrxeBzjg4MYjccxiIizs7OYTaexqKooI672L7QD4uCgPoZQIANCgbwdHdXf5NudxT3Dq4uiiNF4HAeTSRwcHMRwOIyqqqJcLqMsy6jKMqruCKRukxJkwH8yeWs1EV27/2IbDocxKooYj0ZxMB7HZDKJycFBFBd9C8PBIAYRl8NUm5/tcHj5su54hidOTYG8rRpmerENLgr4wWBQ1xaKIg7G4xgOhzEej6O4CIQ0ebM9+afZz2BwORsanjg1BfI1GNTNRxFXl61o/d40ES3m81jM5zGfzWI+m0VVlnUgXBT0g+4+uvvqCx94gtQUyNdgUM8naJaqaNYxKsu6Sal133w6jfPRKOazWcym05gcHsZiPo/lfB7lYhFVex/drXkMMiAUyF8TBE3h3dxufbuvImL69WsMh8OYTqcxOjuLarmMxWIRy/m8DoW+bbm8/CkYyIBQIG/NOkZN4d0OhiYQLgrz5Xwey4uO5+XFHIXqYvRRbyC0g0EokAmhQN6aUGgK7/bw0VYfQBVR1wYGg6iavoSquqwhNMHStzWrpwoFMiAUyFu7A7gdDI2iuDqKKCKqwSBS8d7uM2g3FbXDoCwjzs/r2/DECQXy1cxRaG5HXBbybU3Hc3dyWrczuQmEJgyakKiqevns7n7hCRIK5K07VLQvHJompu6M5/YlObv9B32jkSADQoHnqRsW3Ulpzc+bCv7uDGfIgMlr5O+GNY9W6guAVRPX1BTIiFAgb/c1y1gokAmhQN62qSV09RX43U5oyIRQIH99Hc370oSC4ahkQiiQt74Q2CUk+jqk5/OIs7PNzw0eIaFAvo6Pr/6+KgC2CYZuRzNkQiiQr8PD6xPSdnFT3wJkQiiQr5uCYJdmo+Z38xTIkFAgX5sU/NvWJNQUyIxQIF/tZqNNhqb2XXKz73cT18iQUCBf2wRC202FfTsMlsvNzw0eKaFAvu7jusllGfHp090eA+6RUCBfwxX/3rvWHLprIJm4RkaEAvnbR42hb6SRPgUyJBTI1300HwkEMiMUeD72tQaSi+uQMaFA3lZdeW1b3eYjK6SSGaFAvva1tMWqC+s0tyEjQoF89U1e63vOJro1hel0u3ODR0ooQF8w3NZf0Dz++bPaAlkRCuTp+DiiKOrbu85sbnSbjIQBGRIK5Kko7mZIqnWPyJxQIE93PepIIJApoUCebguBbULipmsqQCaEAnlqX3FtVY1hl2AQCGRKKJCnfV6Cc1UNoSwFA9kRCuRp353MfZPY5nPXUiA7QoG8tYehbjok9bZ5CtOpZbPJjlAgT9vWFNZpDjL6iIwJBfK3zVIWqwp8Hc1kTiiQp76RR9u4aUE8oUCGhALPy12NSIJMCAXydRdXXdN8ROaEAnlbNYltG91AMByVDAkF8rSvZqLmZ3dl1MUi4uxs92PAIyMUyM/BQcTR0X4urNPXRNSe0QyZEQrkZ52mom3nMBh5ROaEAvnpzl6+iw5ntQQyJRTI0za1he41E9q3+zbIkFAgP32BsOulONuEAhkTCuSpr/Df54gkzUdkSiiQn+7chJvCYJuRSGUZ8fXrbucIj5RQIE/7bCbqW/fIHAUyJRTI2777Epqf+hTIlFAgT3c1FFUgkDmhQH720Yew6rrMQoHMCQXytEstoW+OQvt3I4/ImFAgX93RRzcFRXfBu77H1RR4BoQCeVo1ea3vsVVWFf6WzCZjQoH8FMXtz9lmfkJTS/j4URMS2RIK5GUwiDg93d/oo26zkj4FMicUyM+w82+963UVzE/gGREK5GfdzuWb9I060tHMMyAUyM8+ZzF3CQQyJxTIyz76Em66BKeaApkTCuRnOOwfkrrLJTib2zqZyZxQID/dS3Fu0qF809XXIiJms3qDTAkF8rPvvoR2TWE+N3mNrAkF8rTrMNRGdziq/gQyJxTIz75HH5mnwDMiFMjTPkYgrZqnABkTCuSnu/jdpjWH25bOhowJBZ6HVUGxLjUFngmhQH7u4lKcTSCYp0DmhAJ5OTjYraO5b7RRc99yGXF+vp/zhEdKKJCXk5Pr9+1rCe2yNHGN7AkF8rLJJTgbN12Ks11T0HTEMyAUyMsuS2X33W7fp5OZZ0AokJf2BXZ2aTbqLoQnFHgmhAJ5WbUI3i6rpDY/NR/xDAgF8tJdNrsvHG5y27UUIHNCAVbpBsF8/nDnAvdEKJCXdvPRPuYrNLfLMuLnn/dzjvCICQXysq85Cd3bmo54JoQC+dnHsNTu/foUeCaEAnnZ17UUustcCASeCaHA87ZOYS8UeEaEAvlp1xa2XSK7ud0wR4FnQiiQl7u4FGcz+khtgWdAKJCPySSiKK7fv48aw5cvags8C0KBfIzH12c076oJBxPXeCaEAvno1gi2XeKi20yko5lnRCiQj75ZzLusfWQ4Ks+QUCAfq1ZIvU1fwW/ZbJ4poUA+1g2DTZqRBALPjFAgH5teS+Gmwr6vGQmegdFDnwCsMh5v9vxlEZEGje56UZ3mdlXFoFzEaLiIuOV8FgvZwdMnFHhwo1HE8fHVL/rDYcTLl5vt56cYxMd9DkeNiKiqOB5O4813s1uf+uVLPXK13ep0fh4xu/1P4dEQCjyY09N6m0wiDg8vpxhs+yX/0yy2/+OIlX0Jo6KKk8ntf35ycvVPyzJiOq23szOXY+BpEArcq6KI+PbbiBcv6uahoqi3diBEbFeuF2VENM0324xAWnF/Mazi8HCz3TShcHgYsVxGvHoV8d13EV+/Rvz7v9dNTfAYCQXuzatXdRg0gTAc1oHQNBftHArTQUS7sN1TrWE4qOLgYOM/S9tyWQfEeBxxcFA3l338GPHhgz4IHh+hwJ07Oop4/boOg6ZQbGoHTRgML8bB7RIKw2HsFgQR19c8qqoYDKoYrflJ6Q5UKsv6tZZlvTWv/fi4bjp7/z7i8+ftTxf2TShwp05OIr7/vg6G8bg/ENpr2O1Spg9XDbDedhTSxTYcbhYK3Z/NAqtNQAyH9fswGtX9KT/+qL+Bx0MocGdevIh4+7b+VtzuP+irJfTVEDYtywfDGxLlprkKt8xXGFTlVqHQ3G6HQllevuYmxN68qe/78GG9Y8BdEgrcidPTy0BoNxl1A6Gv6Whbg6amcFN146aD9MxRGBdl/PZXHzeeLN3e1WBQ/z4cXoZCt//kzZv6OWoMPDShwN6NRnWTUTsQRqPLjuVuDWHXDubGN4ezGE3Ly77mXZqNLm4PqjJODje/jkK7xtCEQnO7ObVudn33XT189fx889OGfREK7N3r11f7ENr9CN3RRt1RRxHbB8OfvfoYvzt7G5/ad667sxXLZf/mlx96r9uzzq76mpD6rhTaPOfwsH7v/u3fjEri4QgF9ur163oeQrtTudts1G0+um2160383ds/xD/+8De7vYiLEvm3v/yP+MtX77ee8nDTpZ6bfbYDp6rqYbvLZT2XAR6CUGBvRqPrw067tYRuGPR1Mu8SCqcHs/h28jn+Y/mLzf+49dX+sJjFt0dfNq4lNLvp7C4NS101QqrpiB6PL4eqWh6DhyAU2Jvj4zoUVtUOukHQ3SLaP7dLhoOo4u+/+9/x3z8U8eN8i2CIiGJQxt+/+UO8OfoSEZudx2UfQnWlL6Hd0dwOhvZjTYgeHdXv408/bXX6sBOhwF4URT2Cptts1G0yWtXJfDUUBrvVFoaz+K+//F/x397/dXwqT2MatyxcdPF1fjgo42h0Hn/36p/ju8mnnRZZ7dYWmlFHEVdHIA2H9eNFcfm80ahuhvv8ue54hvskFNiLk5O6o3TVkNO+JqO+5qPBYJBqCdvWFiIijooq/uHNP8VP8xfx+/M/id+fv4nlDf/u/+nwXbwZf4jfnv6/i3u2udRI1QqDQVRVFVVVpdpBUyO49led2kJR1E1wJydCgfsnFNiL169vrxms7mAetO4bxmg0iqK4rVBeLzB+NZnFr07/T/x6+jGWVRH/49OfxI+z04iI+Ntv/jVej79ERMTbg59jMlxE3FaruFV1UchXsVyWsVgsrtzXnrzWNDW1m5Wa34uifk/fvzcSifslFNiLmyam9fUfXD4+iKIYpoAYj0cxmRyuEQpttwfEfzn6GhER//nV76Ks6uePh8sYpmVVR7Gfj0N10QxUxnQ6jaoaxmBQXcxmLmMwuOxr6IZDOzzbnfRWVOU+rf0pODz85V2eB09YUXyJ8fj8xs7j7jyEy2/EwxgO65rCaDSKyWQSh4eTGA6L9Nzt9P/h0ZXf9vmd6PLrfFlWMZ3OoiiGsVwOWo8PUgd0s91eWxjHx48bXm0IdrD2p+LVq9/e5XnwhB0c/HOMRuc3jiqKuB4Uw2EdCHVtoYjJZBJHR4cxmUwuagqbJcLqANmh17pXf3tO00Q0m80vwm548TqqqKryyt+ten8irgbD0dFxDIc+e9yfDWoK2w3vu2rfH04eg6L411ubia43GzU1hDoQDg8nMZlMWjWFdvPR5f/NLqOSuvvazHoN+/P5PBaLRQq7ur+kuvb6m36C296n0WgUg8HrLc+Z5227zqi1Q2E8Prn1OTd9YJuhhndrn4XH+se7+2Pd73E2PVZZHqysHfQeJT1vcOX5lyOPLkcgtc9r301J2+9jVU2hvv/yNV0fWtttUusunNd9fDQaxdHRq7XO8HqH9P30UF897t0e8/JYeRzn6rH2fbw7DoWTk9OLW9c/YJf/+IO17r/6WPe+vg/w7fvd5Bz6jtNf4PQ95+Zjrt73Zsdb9zWues/WP2ZE39DP/vv6j/Xu3T/FfN6/b3ZzeHgY33//Z3H7B7y6ZZTS6sf7CqXb77v9/tuOvd1x+/bVvW91897dHG/VMXd5z2/6+3Xf9/XOpWvnnrb1vr11z+b2gn/zY2zv5gJ4M3XHYRU3vZ7WkW893vrv7yYnvM5zr++z/qfa9Fjs7vLzs1khcdvjqwvGa89csfPb97nq3PsL8tvOZd2Cc92CfL1v6euVprsGwCbHvvwsbnseq60dCl+/9l0zcJ+Fg6affR3r/o5XH7MsP699DWM2c35+Hv/yL//3TvZ9n80+V4+XY9PPfTTV3fxtf1/zWdYOhen0y36OSHaKYhbj8eXM3HX/OftWEa2/kVa930zbS09vpr2vu+1ojmi/B1Wn0KiuvOYVq3Vfub1YLGI6/bDlOcPm1g6F8/P3d3kePGEHB+fpegHbbGVZXWxllGU9pHO/AxP2WW26ORzm83mcn09jOp3GdDpLM5uXy/o1dsPgtvdmsZjHdOqzx/1ZOxQ+fPifd3kePGFF8SWOj2NlMLRdfaxZ9qGMsixjuSzTkM6IbWsFD9HfcfkiZ7N5TKfTmM1msVyWF6+xvNiu1wS6ARFx9ZrOX7+excePPnvcnw1qCu/u8jx44ubz1YHQ/r0ZhtkEQqMOhEUsl8vekU+rPa5O7/l8FtPpLMqyDoSmJrSqdtD83g6C5udyGfHTT4tYLn32uD/WPmIvlst6q9f4aTcNtYOg/W24uqgJVBExiPl8GctluXJ8/1Mxn89juWzSrlnzqFoZlt3aVfP+LZd10LaDE+6DUGAv3r2rrxjWFGrdawa0C7fm93roblNrmKXJbE9ZUzto3+7vR7l+u1tLsEIqD0EosBdfvkScn1+u7NleInq5vLyITFPoNatYXIZHPTqn3XT01AKiO+JoVY2gHZzdralxzWb1ewr3TSiwF8tlxA8/XL3QzmBwWej31xSub01B2l0C4mm4fXTRTc1FzbZY1DUvF9jhIQgF9ubsLOLTp7q2sFz2r4XUBET3CmSXcxCqJxoItZs6lNvNQ6tqCYtFxNev9fsID0EosDeLRV2YHR/3X2Gt0QRC9/Huz6eob4hpEwQR/U1F7W0+r6/NPJvd/7lDhFBgz969q5uP3r69Xuh3h6g2fQ59TUdPWd+s5XazUXuUVrvJaDaL+PChboaDhyIU2Lv37yO++eaywG80BWT7IvZNOORQS2jrm6PRHlnUDoT5vN7Oz+tQfapNZ+RBKLB3i0XEH/8Y8f33V+8v6itsXpnE1tQWGrmFQsT1kUfdTuX5vO5H+PHHOhjgIQ2qVWvidp+Yy6eVe3N6WgfD0VHEwUEdCs3WXKC+3YQUcb1T+qm5qemory9hNquD4IcfIn7++eHOm+dhneJeKHCnTk4ug2E8vpzHcFMoPOV/tZvmKPTVEs7O6hrCx48Pd848H0KBR+HoKOIXv4h4+bKuMfQFw6qRSE9Nd55CXy2h6UP4/LnuWP7cd6kSuANCgUflm28iXryow2E8rgOhKK6PQorIIxS6o42a2sHnz/XQ3Q8fdCpzv4QCj05RRHz7bR0O4/HVPoYcmpC6fQndGsLZWd1/cLE6ONwrocCjdnJSh8NkUi+PcdOEt6eiO/x0Oq23szMdyTw8ocCTUBR1QLTDoCjqwHhKvnypawDtYPj6ta4hwGMgFHjSRk9sFs1yqY+Ax22d4v6Jfex4TrS7w/0b3v4UAJ4LoQBAIhQASIQCAIlQACARCgAkQgGARCgAkAgFABKhAEAiFABIhAIAiVAAIBEKACRCAYBEKACQCAUAEqEAQCIUAEiEAgCJUAAgEQoAJEIBgEQoAJAIBQASoQBAIhQASIQCAIlQACARCgAkQgGARCgAkAgFABKhAEAiFABIhAIAiVAAIBEKACRCAYBEKACQCAUAEqEAQCIUAEiEAgCJUAAgEQoAJEIBgEQoAJAIBQASoQBAIhQASIQCAIlQACARCgAkQgGARCgAkAgFABKhAEAiFABIhAIAiVAAIBEKACRCAYBEKACQCAUAEqEAQCIUAEiEAgCJUAAgEQoAJEIBgEQoAJAIBQASoQBAIhQASIQCAIlQACARCgAkQgGARCgAkAgFABKhAEAiFABIhAIAiVAAIBEKACRCAYBEKACQCAUAEqEAQCIUAEiEAgCJUAAgEQoAJEIBgEQoAJAIBQASoQBAIhQASIQCAIlQACARCgAkQgGARCgAkAgFABKhAEAiFABIhAIAiVAAIBEKACRCAYBEKACQCAUAEqEAQCIUAEiEAgCJUAAgEQoAJEIBgEQoAJAIBQASoQBAIhQASIQCAIlQACARCgAkQgGARCgAkAgFABKhAEAiFABIhAIAiVAAIBEKACRCAYBEKACQCAUAEqEAQCIUAEiEAgCJUAAgEQoAJEIBgEQoAJAIBQASoQBAIhQASIQCAIlQACARCgAkQgGARCgAkAgFABKhAEAiFABIhAIAiVAAIBEKACRCAYBEKACQCAUAktG6T6yq6i7PA4BHQE0BgEQoAJAIBQASoQBAIhQASIQCAIlQACARCgAkQgGA5P8D5v1gNtYWjAQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "isDocker = os.environ.get(\"DOCKER_FLAG\") == \"1\"\n",
    "#isDocker = True\n",
    "\n",
    "\n",
    "###Deep q-Learning\n",
    "keras.backend.clear_session()\n",
    "\n",
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # == env.action_space.n\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(32, activation=\"elu\"),\n",
    "    keras.layers.Dense(n_outputs)\n",
    "])\n",
    "\n",
    "liste_fibonnaci = [1,1]\n",
    "actual_fibonnaci = liste_fibonnaci[-1]\n",
    "\n",
    "def next_fibonnaci(liste):\n",
    "    return liste[-1]+liste[-2]\n",
    "    #usefull for rec video\n",
    "\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_memory), size=batch_size)\n",
    "    batch = [replay_memory[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_memory.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "discount_rate = 0.95\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-2)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    next_Q_values = model.predict(next_states)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    target_Q_values = (rewards +\n",
    "                       (1 - dones) * discount_rate * max_next_Q_values)\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "#env.seed(42)\n",
    "#np.random.seed(42)\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "rewards = [] \n",
    "best_score = 0\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.axis(\"off\")\n",
    "renders = []\n",
    "for episode in range(1, num_iters+1):\n",
    "    obs = env.reset()\n",
    "    #print(\"Itteration \", episode)\n",
    "    is_record = False\n",
    "    if episode == actual_fibonnaci:\n",
    "        liste_fibonnaci.append(next_fibonnaci(liste_fibonnaci))\n",
    "        actual_fibonnaci = liste_fibonnaci[-1]\n",
    "        is_record = True\n",
    "    for step in range(num_steps):\n",
    "        epsilon = max(1 - episode / max_epsilon, 0.01)\n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        \n",
    "        if is_record:\n",
    "            #ax.text(20,20, str(episode))\n",
    "            episode_text = ax.text(20, 20, \"Episode \" + str(episode) + \"  Step \" + str(step))\n",
    "            im = ax.imshow(env.render(mode='rgb_array'), animated=True)\n",
    "            renders.append([im, episode_text])\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(step) # Not shown in the book\n",
    "    if step >= best_score: # Not shown\n",
    "        best_weights = model.get_weights() # Not shown\n",
    "        best_score = step # Not shown\n",
    "        #print(\"\\nNew best score: \", best_score)\n",
    "    #print(\"\\rEpisode: {}, Steps: {}, eps: {:.3f}\".format(episode, step + 1, epsilon), end=\"\") # Not shown\n",
    "    if episode > 50:\n",
    "        training_step(batch_size)\n",
    "    env.reset()\n",
    "print(\"\\nBest score: \", best_score)\n",
    "env.close()\n",
    "\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, renders, interval=50, blit=True, repeat=False)\n",
    "ani.save(\"mov.mp4\")\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(rewards)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.show()\n",
    "plt.savefig(\"stats.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1808ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
